{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "https://www.irishtimes.com/life-and-style/people/ireland-s-darkest-oddest-and-weirdest-secrets-uncovered-1.3418059\n",
    "\n",
    "https://www.duchas.ie/download/17.01.26-irish-folklore-and-tradition.pdf\n",
    "\n",
    "https://www.frontiersin.org/articles/10.3389/fphar.2020.584595/full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "https://www.duchas.ie/en/tpc/cbes/5098227?p=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 1\n",
    "base_url = 'https://www.duchas.ie'\n",
    "all_topics = base_url+'/en/tpc/cbes'\n",
    "print(all_topics)\n",
    "page_all = requests.get(all_topics)\n",
    "soup_all = BeautifulSoup(page_all.content, 'html.parser')\n",
    "all_topics_list = soup_all.find_all('ol',{'class':'svelte-5bilvy level1'})[0].find_all('a',href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "#t_ix = 0\n",
    "#curr_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"svelte-5bilvy\" href=\"/en/cbes/stories?TopicID=5098227\"><span class=\"svelte-5bilvy\">agriculture (~2,659)</span> </a>\n",
      "2023-10-19 20:26:11.629091 \tStarting Agriculture\n",
      "\t Agriculture page: 001/136 ... 20 results on this page 2023-10-19 20:26:12.411348\n",
      "['The Schools’ Collection', ' Volume 0002', 'Page\\xa0069']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 58\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mbroken link:\u001B[39m\u001B[38;5;124m\"\u001B[39m,topic_url_pp)\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m school_info_i \u001B[38;5;241m=\u001B[39m \u001B[43msoup_i\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_all\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdiv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mxml\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m     59\u001B[0m page_info_i \u001B[38;5;241m=\u001B[39m soup_i\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m,{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxml\u001B[39m\u001B[38;5;124m'\u001B[39m})[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m     60\u001B[0m title_i \u001B[38;5;241m=\u001B[39m soup_i\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m,{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxml\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     61\u001B[0m                          )[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m“\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m”\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#curr_idx = curr_idx + t_ix + 1\n",
    "#for t_ix, t_url in enumerate(all_topics_list[curr_idx:curr_idx+40]):\n",
    "for t_ix, t_url in enumerate(all_topics_list):\n",
    "    topic_url = t_url['href']\n",
    "    topic_url_i = base_url+topic_url\n",
    "    print(t_url)\n",
    "    page_i = requests.get(topic_url_i)\n",
    "    soup_i = BeautifulSoup(page_i.content, 'html.parser')\n",
    "    #topic_name_i = soup_i.find_all('div',{'class':'pageHeader'})[0].text.replace('\\n','')\n",
    "    topic_name_i = t_url.text.split(' ')[0].capitalize()\n",
    "    #print(topic_name_i)\n",
    "    n_results_topic_i = int(soup_i.find('div',{'class':'count svelte-zzk411'}).text.split(' ')[0].replace(',', ''))\n",
    "    #print(n_results_topic_i)\n",
    "    n_pages_topic_i = int(soup_i.find('form',{'class':'svelte-10ysaoc'}).text.strip().split(' ')[-1])\n",
    "    #print(n_pages_topic_i)\n",
    "    print(dt.datetime.now(),'\\tStarting',topic_name_i)\n",
    "    out_topic_dict = {}\n",
    "\n",
    "#     for pp in range(1,14):\n",
    "    for pp in range(1,n_pages_topic_i+1):\n",
    "        topic_url_pp = base_url+topic_url+'&Page=' + str(pp) + '&PerPage=20'\n",
    "        page_pp = requests.get(topic_url_pp)\n",
    "        soup_pp = BeautifulSoup(page_pp.content, 'html.parser')\n",
    "        try:\n",
    "            all_stories_pp = soup_pp.find('ol',\n",
    "                                        {'class':'svelte-cxmlsf'}).find_all('li',\n",
    "                                                                                {'class':'svelte-cxmlsf'})\n",
    "        except:\n",
    "            print(\"\\t\\tbroken link:\",topic_url_pp)\n",
    "            continue\n",
    "\n",
    "        print('\\t',topic_name_i,'page: %03i/%i ... %i results on this page'%(pp,n_pages_topic_i,\n",
    "                                                        len(all_stories_pp)), dt.datetime.now())\n",
    "#         for sub_story_i in all_stories_pp[:2]:\n",
    "        for sub_story_i in all_stories_pp:\n",
    "            sub_story_url = base_url+sub_story_i.find('a',href=True)['href']\n",
    "            page_i = requests.get(sub_story_url)\n",
    "            soup_i = BeautifulSoup(page_i.content, 'html.parser')\n",
    "            #text_i = soup_i.find_all('div',{'class':'transcript'})\n",
    "            meta_data_list = [dict(zip([i.text for i in x.find_all('dt')],\n",
    "                                       [i.text for i in x.find_all('dd')]))\n",
    "                              for x in soup_i.find_all('dl')]\n",
    "\n",
    "            #print(meta_data_list)\n",
    "\n",
    "            #raw_text_i = \"\\n\\n\".join([i.text for i in\n",
    "                                      #soup_i.find_all('div',{'class':'transcript'})])\n",
    "            try:\n",
    "                # bibl1 = soup_i.find_all('div',{'id':'itemBibli'})[0].find('h2').text\n",
    "                # bibl2 = soup_i.find_all('div',{'id':'itemBibli'})[0].find('p').text\n",
    "                bibl = soup_i.find('p', {'class':'ref svelte-7zi47s'}).text.replace('\\n', '').replace('\\t', '').split(',')\n",
    "                print(bibl)\n",
    "\n",
    "            except:\n",
    "                print(\"\\t\\tbroken link:\",topic_url_pp)\n",
    "                continue\n",
    "\n",
    "            school_info_i = soup_i.find_all('div',{'class':'xml'})[0].text\n",
    "            page_info_i = soup_i.find_all('div',{'class':'xml'})[1].text\n",
    "            title_i = soup_i.find_all('div',{'class':'xml'}\n",
    "                                     )[2].text.replace('“','').replace('”','')\n",
    "            label_id = sub_story_url.replace('https://www.duchas.ie/en/cbes/','')\n",
    "            out_topic_dict[label_id] = {'title':title_i,\n",
    "                                        'bib':[bibl[0].strip(),bibl[1].strip(),bibl[2].strip()],\n",
    "                                        'url':sub_story_url,\n",
    "                                        #'text':raw_text_i,\n",
    "                                        'school_info':school_info_i,\n",
    "                                        'metadata':meta_data_list\n",
    "                                         }\n",
    "\n",
    "    if not os.path.exists('/data/out/'):\n",
    "        os.mkdir('/data/out/')\n",
    "\n",
    "    with open('/data/out/schools-collection_%s.json'%topic_name_i, 'w') as fp:\n",
    "        json.dump(out_topic_dict, fp)\n",
    "\n",
    "    print(dt.datetime.now(),'\\tSaving....',topic_name_i,t_ix,'\\n')\n",
    "\n",
    "    all_data[topic_name_i] = out_topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
